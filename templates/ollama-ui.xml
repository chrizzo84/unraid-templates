<?xml version="1.0"?>
<Container version="2">
  <Name>OllamaUI</Name>
  <Repository>ghcr.io/chrizzo84/ollamaui</Repository>
  <Registry>https://github.com/chrizzo84/OllamaUI</Registry>
  <Network>bridge</Network>
  <MyIP/>
  <Shell>sh</Shell>
  <Privileged>false</Privileged>
  <Support>https://github.com/chrizzo84/OllamaUI/issues</Support>
  <Project>https://github.com/chrizzo84/OllamaUI</Project>
  <Overview>OllamaUI: A Unified Experience for Local Large Language Models
OllamaUI represents more than just a user interface; it is the integration of the core Ollama service with a clean and user-friendly graphical interface. This combination provides a seamless platform for managing and interacting with local large language models.
With OllamaUI, you can effortlessly browse both locally installed and remote models. The interface allows for pulling new model variants with clear progress feedback and provides tools to organize your AI workflows efficiently in one central location. It is an ideal solution for anyone seeking a straightforward and effective way to utilize the power of Ollama.
A key feature is the ability to manage other Ollama instances, as the host connection is fully configurable. By setting the the host in the UI you can connect to any Ollama server on your network, allowing for centralized management of multiple, distributed instances.
The underlying Ollama engine continues to function just as it does in the standard container setup, ensuring a consistent and familiar core experience for existing users. The UI acts as an accessible and intuitive layer, simplifying model management and interaction without altering the fundamental capabilities of Ollama.</Overview>
  <Category>AI:</Category>
  <WebUI>http://[IP]:[PORT:3000]</WebUI>
  <TemplateURL/>
  <Icon>https://raw.githubusercontent.com/chrizzo84/unraid-templates/main/icons/ollama_ui.png</Icon>
  <ExtraParams>--gpus all</ExtraParams>
  <PostArgs/>
  <CPUset/>
  <DateInstalled>1754848730</DateInstalled>
  <DonateText/>
  <DonateLink/>
  <Requires>For GPU acceleration, the 'Nvidia-Driver' plugin from Community Apps must be installed.</Requires>
  <Config Name="Web UI Port" Target="3000" Default="3000" Mode="tcp" Description="Port for the Web UI" Type="Port" Display="always" Required="true" Mask="{8}">3000</Config>
  <Config Name="Ollama API Port" Target="11434" Default="11434" Mode="tcp" Description="Port for the Ollama API" Type="Port" Display="always" Required="true" Mask="{8}">11434</Config>
  <Config Name="Ollama Models Path" Target="/root/.ollama" Default="/mnt/user/appdata/OllamaUI/ollama_models" Mode="rw" Description="Storage location for the downloaded Ollama models." Type="Path" Display="always" Required="true" Mask="{8}">/mnt/user/appdata/OllamaUI/ollama_models</Config>
  <Config Name="UI Database Path" Target="/app/data" Default="/mnt/user/appdata/OllamaUI/ui_database" Mode="rw" Description="Storage location for the user interface database. (Corrected path)" Type="Path" Display="always" Required="true" Mask="{8}">/mnt/user/appdata/OllamaUI/ui_database</Config>
  <Config Name="PUID" Target="PUID" Default="99" Mode="" Description="User ID to run the container as. Matches Unraid's 'nobody' user." Type="Variable" Display="advanced" Required="false" Mask="{8}">99</Config>
  <Config Name="PGID" Target="PGID" Default="100" Mode="" Description="Group ID to run the container as. Matches Unraid's 'users' group." Type="Variable" Display="advanced" Required="false" Mask="{8}">100</Config>
  <TailscaleStateDir/>
</Container>